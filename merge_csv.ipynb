{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running each cell, be sure to change names of files to use and files to be generated, alongside how many cores were used and how m any files are in range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "#########################################################\n",
    "# This script merges all the training data files csv's \n",
    "# generated inside each core for each race simulation \n",
    "# specified, into one single csv for that particular core. \n",
    "\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# Define the base path and core directories\n",
    "base_path = \"/Users/joesouber/XGBoost_TBBE\"\n",
    "cores = [\"Core1\", \"Core2\", \"Core3\", \"Core4\", \"Core5\", \"Core6\", \"Core7\", \"Core8\"]\n",
    "\n",
    "for core in cores:\n",
    "    # Generate filenames programmatically for each core\n",
    "    files = [os.path.join(base_path, core, \"TBBE_OD_XGboost\", \"Application\", \"getXGBOOstTrainingData_{}.csv\".format(i)) for i in range(400)]\n",
    "    \n",
    "    # Specify the merged file location for each core\n",
    "    merged_filename = os.path.join(base_path, core, \"TBBE_OD_XGboost\", \"Application\", \"merged_result_2_{}.csv\".format(core))\n",
    "    \n",
    "    with open(merged_filename, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for index, filename in enumerate(files):\n",
    "            try:\n",
    "                with open(filename, 'r') as infile:\n",
    "                    reader = csv.reader(infile)\n",
    "                    \n",
    "                    # Skip header only if it's not the first file\n",
    "                    if index != 0:\n",
    "                        next(reader, None)\n",
    "                        \n",
    "                    # Write rows from current file to the output file\n",
    "                    writer.writerows(reader)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {filename} not found and will be skipped.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {e} - {filename} could not be processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# This script merges all those merged files above for each core into one single csv file for the entire simulation.\n",
    "#####################\n",
    "\n",
    "\n",
    "# Define the base path and the cores\n",
    "base_path = \"/Users/joesouber/XGBoost_TBBE\"\n",
    "cores = [\"Core1\", \"Core2\", \"Core3\", \"Core4\", \"Core5\", \"Core6\", \"Core7\", \"Core8\"]\n",
    "\n",
    "# Generate filenames for the merged core files\n",
    "merged_files = [os.path.join(base_path, core, \"TBBE_OD_XGboost\", \"Application\", \"merged_result_2_{}.csv\".format(core)) for core in cores]\n",
    "\n",
    "# Specify the final merged file location\n",
    "final_merged_filename = os.path.join(base_path, \"final_merged_result_2.csv\")\n",
    "\n",
    "with open(final_merged_filename, 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    for index, filename in enumerate(merged_files):\n",
    "        try:\n",
    "            with open(filename, 'r') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                \n",
    "                # Skip header only if it's not the first file\n",
    "                if index != 0:\n",
    "                    next(reader, None)\n",
    "                    \n",
    "                # Write rows from current file to the output file\n",
    "                writer.writerows(reader)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {filename} not found and will be skipped.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error: {e} - {filename} could not be processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# This script is to be used when an entirely new simulation has been run and the two functions above have been used. \n",
    "# It merges the old combined simulation csv with the newly generated one, so one csv of all simulations is formed.\n",
    "#####################\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths to the files to be merged\n",
    "file1 = \"/Users/joesouber/XGBoost_TBBE/final_merged_result_1.csv\"\n",
    "file2 = \"/Users/joesouber/XGBoost_TBBE/final_merged_result_2.csv\"\n",
    "\n",
    "# Define the path to the final merged file\n",
    "final_merged_filename = \"/Users/joesouber/XGBoost_TBBE/final_merged_combined.csv\"\n",
    "\n",
    "# Function to merge the files\n",
    "def merge_files(file_list, output_file):\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for index, filename in enumerate(file_list):\n",
    "            try:\n",
    "                with open(filename, 'r') as infile:\n",
    "                    reader = csv.reader(infile)\n",
    "                    \n",
    "                    # Skip header only if it's not the first file\n",
    "                    if index != 0:\n",
    "                        next(reader, None)\n",
    "                        \n",
    "                    # Write rows from current file to the output file\n",
    "                    writer.writerows(reader)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {filename} not found and will be skipped.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {e} - {filename} could not be processed.\")\n",
    "\n",
    "# Merge the specified files\n",
    "merge_files([file1, file2], final_merged_filename)\n",
    "\n",
    "print(f\"Merged files into {final_merged_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for training data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# This script extracts the top 20% and bottom 20% of the\n",
    "# dataset based on the 'balance' column and assigns a label\n",
    "# of 1 to the top 20% and 0 to the bottom 20%. The new\n",
    "# dataset is arranged by time and saved to a CSV file.\n",
    "#########################################################\n",
    "\n",
    "\n",
    "def extract_training_data(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Sort the DataFrame by the 'balance' column in descending order\n",
    "    df_sorted = df.sort_values(by='balance', ascending=False)\n",
    "\n",
    "    # Determine the number of rows that constitute the top 20% and bottom 20%\n",
    "    top_20_count = int(len(df_sorted) * 0.2)\n",
    "    bottom_20_count = int(len(df_sorted) * 0.2)\n",
    "\n",
    "    # Extract the top 20% and bottom 20% of the sorted DataFrame\n",
    "    top_20_df = df_sorted.head(top_20_count)\n",
    "    bottom_20_df = df_sorted.tail(bottom_20_count)\n",
    "\n",
    "    # Assign label 1 to the top 20% and label 0 to the bottom 20%\n",
    "    top_20_df['label'] = 1\n",
    "    bottom_20_df['label'] = 0\n",
    "\n",
    "    # Combine the top and bottom DataFrames\n",
    "    combined_df = pd.concat([top_20_df, bottom_20_df])\n",
    "\n",
    "    # Sort the combined DataFrame by the 'time' column\n",
    "    combined_df = combined_df.sort_values(by='time')\n",
    "\n",
    "    # Ensure 'label' column is the penultimate column and 'decision' is the last column\n",
    "    cols = list(combined_df.columns)\n",
    "    cols.remove('label')\n",
    "    cols.insert(-1, 'label')\n",
    "    combined_df = combined_df[cols]\n",
    "\n",
    "    # Save the new dataset to a CSV file\n",
    "    combined_df.to_csv('new_dataset.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python extract_training_data.py <file_path>\")\n",
    "    else:\n",
    "        file_path = sys.argv[1]\n",
    "        extract_training_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "#########################################################\n",
    "# This script preprocesses the data by dropping the 'type'\n",
    "# column, mapping the 'decision' column to 1 and 0, and\n",
    "# performing MinMax scaling. The preprocessed data is saved\n",
    "# to a CSV file and optionally uploaded to an S3 bucket.\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(input_path, output_path, bucket_name):\n",
    "    # Load data\n",
    "    data = pd.read_csv(input_path)\n",
    "    \n",
    "    # Drop 'type' column and NaNs\n",
    "    data.drop(columns=['type'], inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Map 'decision' column\n",
    "    data['decision'] = data['decision'].map({'backer': 1, 'layer': 0})\n",
    "    \n",
    "    # MinMax Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    output_file = os.path.join(output_path, 'preprocessed_data.csv')\n",
    "    data.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(output_file, bucket_name, 'preprocessed-output/preprocessed_data.csv')  # Ensure this path is correct\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, required=True)\n",
    "    parser.add_argument('--output-path', type=str, required=True)\n",
    "    parser.add_argument('--bucket-name', type=str, required=True)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    preprocess_data(args.input_path, args.output_path, args.bucket_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
